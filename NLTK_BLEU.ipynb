{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX79iDN_6xO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4331428-45ef-4853-f660-26c91bcfda56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VopY3sShdecN",
        "outputId": "e14239f7-ccbc-498a-db89-2b08aa3e8962"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLujb7SaWXjY"
      },
      "source": [
        "import spacy\n",
        "nlp1 = spacy.load('/content/drive/MyDrive/hindi_lang/model-final')\n",
        "\n",
        "file1=\"/content/reference.txt\"\n",
        "file2=\"/content/ref_pos\"\n",
        "file3=\"/content/ref_dep\"\n",
        "lines = [line.rstrip('\\n') for line in open(file1)]\n",
        "\n",
        "file_out=open(file2,\"w\")\n",
        "file_out1=open(file3,\"w\")\n",
        "for each_line in lines:\n",
        "    doc = nlp1(each_line)\n",
        "    #print(each_line)\n",
        "    for token in doc:\n",
        "        file_out.write(str(token.tag_))\n",
        "        file_out.write(\" \")\n",
        "        file_out1.write(str(token.dep_))\n",
        "        file_out1.write(\" \")\n",
        "    file_out.write(\"\\n\")\n",
        "    file_out1.write(\"\\n\")\n",
        "file_out.close()\n",
        "file_out1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NWdwRgO_U4P"
      },
      "source": [
        "**SENETENCE WISE BLEU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT3LaZkoxd9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8970ad-a6d5-4c0c-f305-54e93e2d8f28"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "translated_file = \"/content/drive/MyDrive/PhD/projectfolder/LM/Outputs/without_LM/En_Hi\"\n",
        "reference_file = \"/content/drive/MyDrive/PhD/projectfolder/LM/Outputs/hi_1000.txt\"\n",
        "f5 = open('/content/BLEU.txt', \"a+\")\n",
        "with open(translated_file, \"r\") as f1, open(reference_file, \"r\") as f2:\n",
        "  lines1 = [line.rstrip('\\n') for line in f1]\n",
        "  #print(lines1)\n",
        "  lines2 = [line.rstrip('\\n') for line in f2]\n",
        "  #print(lines2)\n",
        "  for l1,l2 in zip(lines1,lines2):\n",
        "    tokens_ref=[word_tokenize(l2)]\n",
        "    tokens_translate=word_tokenize(l1)\n",
        "    score = sentence_bleu(tokens_ref, tokens_translate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    f5.write(str(score)+'\\n')\n",
        "f5.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmT0w5vvHI8"
      },
      "source": [
        "**CORPUS BLEU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWhXIDk3Zq-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64639160-7011-465d-f9ef-caf7ac220afb"
      },
      "source": [
        "import nltk\n",
        "weights='0.25 0.25 0.25 0.25'\n",
        "weight = [float(v) for v in weights.split()]\n",
        "with open(\"/content/drive/MyDrive/PhD/projectfolder/LM/Outputs/kenlm/3L_2/en_hi/En_hn_kenlm_6\", 'r') as trans, open(\"/content/drive/MyDrive/PhD/projectfolder/LM/Outputs/hi_1000.txt\", 'r') as ref:\n",
        "            tran_list, ref_list = [], []\n",
        "            for tl in trans:\n",
        "                rl = ref.readline()\n",
        "                tran_list.append(tl.strip().split(' '))\n",
        "                ref_list.append(rl.strip().split(' '))\n",
        "print(nltk.translate.bleu_score.corpus_bleu(ref_list, tran_list, weight))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.220486919374789e-156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOznfvx9nzC_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}